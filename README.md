# Serie

This repo aims to automatically collect, filter, process, and save papers from arXiv, OpenReview, and other sources, focusing on the latest research.

## Setup

```bash
conda create -n YourEnvName python=3.12
conda activate YourEnvName
[uv] pip install -e .
```

## Getting Started

The `serie/main.py` is the entry point, all available arguments are defined in `serie/config.py` and can be listed by:

```bash
python serie/main.py --help
```

Let's first take a look at how it works, and how to collect papers from arXiv.

### How it Works

This repository uses a pipeline to collect papers from various resources, each pipeline is consisting of multiple plugins. The pipelines and plugins are configurable. For example, [Request](configs/pipelines/request.py) defined in `configs/pipelines/request.py` specifies how to collect papers from arXiv, how to filter them, and how to save them.

```python
config = {
    "Request": {  # Defines which pipeline to be used, there are defined in serie/pipelines
        "plugins": [  # Specifies the plugins to be used in this pipeline, there are defined in serie/plugins
            "ResultLoader",
            "ArxivParser",  # CVF and OpenReview are also supported
            "GitHubLinkParser",  # Using regex to parse GitHub links from the paper abstract
            "DefaultKeywordsFilter",  # Filter papers based on keywords defined in configs/plugins/default_keywords_filter.py
            "LanguageModelBasedKeywordsFilter",  # Filter papers based on keywords by language model. Comment this line if you don't want to use language model.
            "MarkdownTableMaker",  # Make a markdown table of the papers
            "DownloaderInformationCollector",
            "ResultSaver",  # Save the results to a markdown file before translating the papers.
            "Translator",
            "ResultSaver",  # Save the results to a markdown file after translating the papers.
            "DownloadedPaperIndexGenerator"
        ],
        "configs": {  # Specifies how to override the default configurations of the plugins, default configurations are defined in configs/plugins
            "ResultSaver": {
                "keywords_filter_plugin": "LanguageModelBasedKeywordsFilter"
            },
            "Translator": {
                "keywords_filter_plugin": "LanguageModelBasedKeywordsFilter"
            }
        }
    }
}
```

### Collect Papers from arXiv

#### Run without Language Model

If you don't want to use language model, you can comment out the `LanguageModelBasedKeywordsFilter` and `Translator` plugin in the pipeline configuration, e.g., [Request](configs/pipelines/request.py).

Then, check the configuration of each plugin in `configs/plugins`, and modify them according to your needs, e.g., where to save the results.

For example, the default keywords filter plugin is `DefaultKeywordsFilter`, which filters papers based on keywords defined in `configs/plugins/default_keywords_filter.py`. For example:

```python
keywords = {
    "detect": ["detect", "detection"],
    "segment": ["segment", "segmentation"],
}
```

It will filter papers that contain any of the keywords in the `detect` or `segment` categories, and save them to the specified output directory (specified by `markdown_directory` in [ResultSaver](configs/plugins/result_saver.py)) with the filename `papers @ detect.md` and `papers @ segment.md`.

Finally, run:

```bash
python serie/main.py --pipeline "Request"
```

By default, the pipeline will collect papers of yesterday (arxiv). You can specify the date by setting the `datetime` argument, e.g., `--datetime 2023-10-01`.

```bash
python serie/main.py --pipeline "Request" --datetime 2023-10-01
```

If you are familiar with the query of arXiv API, you can also specify the query by setting the `query` argument.

You will see:

1. A folder named with the current date in `outputs/`, e.g., `outputs/20231001`:
    - A log file generated by the last run, e.g., `outputs/20231001/2023-10-01-11-45-21.log`.
    - A `papers.jsonl` that contains all the information of the papers that can be loaded by `ResultLoader`.
    - A `papers.txt` that contains all the paper information saved in a text format.
2. A folder with the name specified by `markdown_directory` in `ResultSaver`:
    - A `papers.md` file that contains all the papers after filtering by the keywords defined in `DefaultKeywordsFilter`.
    - A `papers @ detect.md` file that contains all the papers that contain any of the keywords in the `detect` category.
    - A `papers @ segment.md` file that contains all the papers that contain any of the keywords in the `segment` category.

#### Run with Language Model

If you want to use language model, you need to set the `LanguageModelBasedKeywordsFilter` and `Translator` plugins in the pipeline configuration, e.g., [Request](configs/pipelines/request.py).

Then, specify the API key of the language model in the environment variable, e.g., `SILICONFLOW_APIKEY` (or some other models you want to use) specified in `configs/core/agent.py`:

```bash
export SILICONFLOW_APIKEY="your_apikey"
```

Then, check the model name in `LanguageModelBasedKeywordsFilter` and `Translator` plugins, e.g., `configs/plugins/language_model_based_keywords_filter.py` and `configs/plugins/translator.py`, and modify them according to your needs.

Other configurations are similar to the previous section, e.g., where to save the results.

Finally, run:

```bash
python serie/main.py --pipeline "Request"
```

### Download Papers from arXiv

Fill the necessary `download.json` file in the `configs/misc/`, e.g.,

```json
[
    {
        "url": "http://arxiv.org/abs/2503.08507v1",
        "tags": ["RexSeek", "detection", "vision-language", "HumanRef", "dataset", "benchmark"],
        "venue": "arxiv",
        "local_plugin_data": {
            "Downloader": {"download": true, "subdir": "Vision-Language"}
        }
    }
]
```

The `url` is the paper URL, `tags` are the self-defined tags/keywords of this paper, `venue` is the venue of the paper, and "Downloader" is the plugin data for the `Downloader` plugin, which specifies whether to download the paper and where to save it.

Then, check the configuration of the `Downloader` pipeline in `configs/pipelines/download.py`, and modify it according to your needs, e.g., where to save the downloaded papers.

Finally, run:

```bash
python serie/main.py --pipeline "Download"
```

## Schedule Request Task

```bash
nohup ./request.sh >> outputs/nohup.out &

```

## TODO

- [ ] Better README.
- [ ] Support specifying custom prompt for language model in `configs/`.
- [ ] Improve the code quality.
- [ ] Improve the usability of the pipeline and plugins.
- [ ] `multiprocessing` support of OpenReview and CVF.
